{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Protein Atlas.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "LbWCSZXTudM0"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "f0bjPvkOssWq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import time\n",
        "import os\n",
        "import zipfile\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "c-nCr7GiGMVS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#constants\n",
        "\n",
        "checkpoint_dir = 'model_ckpt'\n",
        "tr_summaries_dir = os.path.join(checkpoint_dir,'training_summary')\n",
        "val_summaries_dir = os.path.join(checkpoint_dir,'validation_summary')\n",
        "ts_summaries_dir = os.path.join(checkpoint_dir,'testing_summary')\n",
        "\n",
        "validation_start_partition = 0.9\n",
        "test_start_partition = 0.95"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GxvWbivvJ4MI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Preprocessing stuff\n",
        "\n",
        "### TODO: Modify methods to preprocess image and label."
      ]
    },
    {
      "metadata": {
        "id": "1gzqr71StMcO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def extract_datazip(zipfile_path='gdrive/My Drive/img_align_celeba.zip', validation_start_id=None, test_start_id=None, extract_path='celebA'):\n",
        "  \"\"\"\n",
        "  Extract dataset zip file at 'zipfile_path' to train, valid and test directory in 'extract_path'.\n",
        "  Creating directory structure:\n",
        "  extract_path/\n",
        "    train/img_align_celeba/\n",
        "    valid/img_align_celeba/\n",
        "    test/img_align_celeba/\n",
        "    \n",
        "  return:\n",
        "    list, list, list : list of files in train, valid and test directories\n",
        "    dict : dict of paths \n",
        "  \n",
        "  \"\"\"\n",
        "  \n",
        "  zfile = zipfile.ZipFile(zipfile_path)\n",
        "  zlist = zfile.namelist()[1:]\n",
        "  \n",
        "  if validation_start_id ==None:\n",
        "    validation_start_id = int(len(zlist) * validation_start_partition)\n",
        "  if test_start_id ==None:\n",
        "    test_start_id = int(len(zlist) * test_start_partition)\n",
        "    \n",
        "  zlist_train = zlist[ : validation_start_id]\n",
        "  zlist_valid = zlist[validation_start_id : test_start_id]\n",
        "  zlist_test = zlist[test_start_id : ]\n",
        "  \n",
        "  print('Extracting train images at {}'.format(extract_path), end='\\t')\n",
        "  zfile.extractall(os.path.join(extract_path, 'train'), zlist_train)\n",
        "  print('done')\n",
        "  print('Extracting validation images at {}'.format(extract_path), end='\\t')\n",
        "  zfile.extractall(os.path.join(extract_path, 'valid'), zlist_valid)\n",
        "  print('done')\n",
        "  print('Extracting test images at {}'.format(extract_path), end='\\t')\n",
        "  zfile.extractall(os.path.join(extract_path, 'test'), zlist_test)\n",
        "  print('done')\n",
        "  return zlist_train, zlist_valid, zlist_test, {'train': os.path.join(extract_path,'train'), 'valid': os.path.join(extract_path,'valid'), 'test': os.path.join(extract_path,'test')}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ESxw1Nzs0NUn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def label_from_file(filepath='gdrive/My Drive/list_attr_celeba.txt', validation_start_id=None, test_start_id=None):\n",
        "  \"\"\"\n",
        "  decode attribute file into train, validation, test labels.\n",
        "  \n",
        "  parameters:\n",
        "    filepath: path to csv file\n",
        "    validation_start_id : index of start of validation label in csv file\n",
        "    test_start_id : index of start of test label in csv file\n",
        "  \n",
        "  return:\n",
        "    dict : dict of list containing (index, label) pairs.\n",
        "  \n",
        "  \"\"\"\n",
        "  \n",
        "  lbl = pd.read_csv('gdrive/My Drive/list_attr_celeba.txt', sep='\\s+', skiprows=1)\n",
        "  lbl = lbl.replace(to_replace=-1, value=0)\n",
        "  lbl_values = lbl.as_matrix()\n",
        "  lbl_indexes = lbl.index.get_values()\n",
        "  lbl_size = lbl_values.shape[0]\n",
        "  \n",
        "  if validation_start_id == None:\n",
        "    validation_start_id = int(lbl_size * validation_start_partition)\n",
        "  if test_start_id == None:\n",
        "    test_start_id = int(lbl_size * test_start_partition)\n",
        "  \n",
        "  lbl_train = lbl_values[ : validation_start_id]\n",
        "  lbl_valid = lbl_values[validation_start_id : test_start_id]\n",
        "  lbl_test  = lbl_values[test_start_id : ]\n",
        "\n",
        "  index_train = lbl_indexes[ : validation_start_id]\n",
        "  index_valid = lbl_indexes[validation_start_id : test_start_id]\n",
        "  index_test  = lbl_indexes[test_start_id : ]\n",
        "  \n",
        "  return {'train':[index_train, lbl_train], 'valid': [index_valid, lbl_valid], 'test': [index_test, lbl_test]}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Wbdj-amsxBQV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def gen_batch(image_dir_path='celebA/train', lbl_list=None, batch_size=32):\n",
        "  '''\n",
        "  Returns a one shot iterator for dataset, which when run through session produces [img, img_file_name], img_label\n",
        "  '''\n",
        "  def decode(filename):\n",
        "    raw_file = tf.io.read_file(filename)\n",
        "    img = tf.divide(tf.image.decode_jpeg(raw_file, channels=3),255)\n",
        "    return tf.reshape(img, shape=[218, 178, 3])\n",
        "  \n",
        "  assert not lbl_list == None, \"lbl_list not referenced\"  \n",
        "  assert len(lbl_list) ==2, \"lbl_list should have index and label\"\n",
        "  \n",
        "  img_list = tf.matching_files(image_dir_path+'/img_align_celeba/*.jpg')\n",
        "  dataset = tf.data.Dataset.from_tensor_slices((img_list, lbl_list[0], lbl_list[1]))\n",
        "  dataset = dataset.shuffle(buffer_size=len(lbl_list))\n",
        "  dataset = dataset.map(lambda x,y,z : [decode(x), y, tf.cast(z, dtype= tf.uint8) ])\n",
        "  dataset = dataset.batch(batch_size).repeat()\n",
        "  return dataset.make_one_shot_iterator()\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kyIxxqEcxLht",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Model Definition"
      ]
    },
    {
      "metadata": {
        "id": "i-85StyeZmaj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_model(X, output_units):\n",
        "  \"\"\"\n",
        "  returns tensorflow model network \n",
        "  \n",
        "  parameters:\n",
        "  X : tensor, input to the network\n",
        "  output_units: \n",
        "  \"\"\"\n",
        "  sig_cond = tf.Variable([0.5]*output_units)\n",
        "  \n",
        "  layer = tf.layers.conv2d(inputs=X, filters=16, kernel_size=[3,3], kernel_initializer= tf.initializers.random_uniform(),  )\n",
        "  layer = tf.layers.max_pooling2d(inputs = layer, pool_size=[2,2], strides=2)\n",
        "  layer = tf.nn.relu(layer)\n",
        "  layer = tf.layers.conv2d(inputs=layer, filters=64, kernel_size=[3,3], kernel_initializer= tf.initializers.random_uniform(), )\n",
        "  layer = tf.layers.max_pooling2d(inputs = layer, pool_size=[2,2], strides=2)\n",
        "  layer = tf.nn.relu(layer)\n",
        "  layer = tf.layers.flatten(inputs = layer,)\n",
        "  layer = tf.layers.dense(inputs = layer, units=256, kernel_initializer=tf.initializers.random_uniform(), )\n",
        "  layer = tf.nn.relu(layer)\n",
        "  layer = tf.layers.dense(inputs= layer, units = 128, kernel_initializer=tf.initializers.random_uniform(), )\n",
        "  layer = tf.nn.relu(layer)\n",
        "  logit =  tf.layers.dense(inputs= layer, units = output_units, kernel_initializer=tf.initializers.random_uniform(), )\n",
        "  Y_ = tf.nn.sigmoid(logit,)\n",
        "  Y_ = tf.cast(x = tf.greater(tf.cast(Y_, tf.float32), sig_cond), dtype=tf.int8,  name = 'predictions')\n",
        "  return logit, Y_"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_tFZm0LiFMSy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Graph Definition"
      ]
    },
    {
      "metadata": {
        "id": "m8KsvlFy4lcd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_graph(graph_type=None, summaries_dir=None, *args, **kwargs):\n",
        "  \n",
        "  # argument validation\n",
        "  assert graph_type in ['train', 'valid', 'test'], 'Invalid graph type'\n",
        "  assert summaries_dir !=None, 'summary directory not defined'\n",
        "  if graph_type=='train':\n",
        "    assert 'lr' in kwargs.keys() != None, 'training graph must have learing rate' \n",
        "  \n",
        "  graph = tf.Graph()  \n",
        "  with graph.as_default():\n",
        "    with tf.variable_scope('training_graph'):\n",
        "      #datasets:\n",
        "      iterator = gen_batch(image_dir_path=path_dict[graph_type], lbl_list=lbl[graph_type])\n",
        "      batch = iterator.get_next()\n",
        "      saveable = tf.contrib.data.make_saveable_from_iterator(iterator)\n",
        "      # TODO: figure out how to save iterator state\n",
        "      #  ::::HERE::::\n",
        "      X = batch[0]\n",
        "      Y = tf.stop_gradient(batch[2])      \n",
        "\n",
        "      #model\n",
        "      logit, predictions = get_model(X, Y.shape[-1])    \n",
        "\n",
        "      #ops\n",
        "      loss_op = tf.reduce_mean( tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.cast(Y, tf.float32), logits=logit))\n",
        "      accuracy_op, update_op = tf.metrics.accuracy( labels=Y, predictions=Y_, name='accuracy')\n",
        "      acc_mat_init = tf.variables_initializer( graph.get_collection('local_variables', scope='training_graph/accuracy'))\n",
        "\n",
        "      if type == 'train':\n",
        "        #optimiser\n",
        "        #TODO: implement adaptive learning rate method \n",
        "        #  ::::HERE::::\n",
        "        global_step = tf.Variable(0, name='global_step')\n",
        "        optimizer = tf.train.AdamOptimizer( learning_rate=kwargs['lr'] )\n",
        "        train_op = optimizer.minimize(loss_op, global_step=global_step)\n",
        "      else:\n",
        "        train_op = None\n",
        "          \n",
        "\n",
        "      # summary and checkpoint\n",
        "      tf.summary.scalar('loss', loss_op)\n",
        "      tf.summary.scalar('accuracy', accuracy_op)\n",
        "      # TODO: add other summary items eg. learning rate\n",
        "      #  :::HERE:::\n",
        "      \n",
        "      summaries = tf.summary.merge_all()\n",
        "      summary_writer = tf.summary.FileWriter( summaries_dir, graph )\n",
        "      config_proto = tf.ConfigProto(allow_soft_placement=True, **kwargs)\n",
        "      session = tf.Session( config= config_proto )\n",
        "      \n",
        "      return {'logits':logit, \n",
        "              'predictions':predictions, \n",
        "              'loss':loss_op, \n",
        "              'train_op':train_op, \n",
        "              'accuracy':accuracy_op, \n",
        "              'update_acc':update_op, \n",
        "              'acc_mat_init':acc_mat_init, \n",
        "              'summaries':summaries,\n",
        "              'summary_writer':summary_writer, \n",
        "              'session':session, }"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TU3cTcXUGlcv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Train the model"
      ]
    },
    {
      "metadata": {
        "id": "oHaqXL9JuTcT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train_model():\n",
        "  # TODO: write training loop\n",
        "  pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XGmk0Xd8xh18",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Test Model"
      ]
    },
    {
      "metadata": {
        "id": "gEkhUlzPxjoe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def test_model():\n",
        "  # TODO: write testing loop\n",
        "  pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LbWCSZXTudM0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##STUFF \n",
        "###testing area"
      ]
    },
    {
      "metadata": {
        "id": "ict4Urjm6KSF",
        "colab_type": "code",
        "outputId": "833f45eb-26ec-4ada-a32a-efbac427c5b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# testing step\n",
        "with tf.device(\"/device:GPU:0\"):\n",
        "  with tf.name_scope('test_loop'):\n",
        "    num_ts_batches = ts_x.shape[0] //batch_size\n",
        "    with tf.Session(graph=graph, config=tf.ConfigProto(allow_soft_placement=True,log_device_placement=True)) as sess:\n",
        "      saver.restore(sess=sess, save_path= tf.train.latest_checkpoint( checkpoint_dir=checkpoint_dir))\n",
        "      sess.run([running_variables_initializer])\n",
        "      for batch_inst in range(num_ts_batches):\n",
        "        ts_batch= sess.run(test_batch)\n",
        "        loss, _, accuracy = sess.run([loss_op, update_op, accuracy_op], feed_dict={X:ts_batch[0], Y:ts_batch[1]})\n",
        "      print(\"loss: {:.6f}, accuracy:{:.6f}\".format(loss,accuracy))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "img_align_celeba\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "yZM58bEY9P9d",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "with tf.Session() as sess:\n",
        "  refr = tf.Variable([0.5]*40)\n",
        "  sess.run( tf.global_variables_initializer())\n",
        "  tr_l = sess.run([train_batch[2],tf.cast(tf.greater(tf.cast(train_batch[2], tf.float32), refr), tf.int8)])\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "czn16SI09uzt",
        "colab_type": "code",
        "outputId": "568711bd-761e-4f46-b0ad-7757d219ae96",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "cell_type": "code",
      "source": [
        "tr_l[0][0],tr_l[1][0] "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1,\n",
              "        0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1], dtype=uint8),\n",
              " array([0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1,\n",
              "        0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1], dtype=int8))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "metadata": {
        "id": "Qa2mWJe_NwXR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Run"
      ]
    },
    {
      "metadata": {
        "id": "8IDxNdDdszGa",
        "colab_type": "code",
        "outputId": "365b897c-2bd5-49ab-f249-0d02ad43658d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "cell_type": "code",
      "source": [
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "IixhO1-YVJR0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "validation_start_id, test_start_id= get_dataset_partition_index()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TsG_oBhJsE1H",
        "colab_type": "code",
        "outputId": "4d437a93-9646-4bce-98ea-d228d5661f17",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "cell_type": "code",
      "source": [
        "list_train, list_valid, list_test, path_dict = extract_datazip(validation_start_id=validation_start_id, test_start_id=test_start_id)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Extracting train images at celebA\tdone\n",
            "Extracting validation images at celebA\tdone\n",
            "Extracting test images at celebA\tdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "lVPc67q_G376",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "lbl = label_from_file(validation_start_id=validation_start_id, test_start_id=test_start_id)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hby9lYZ20kyo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_model()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZTSx2CNA0uUD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "test_model()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}