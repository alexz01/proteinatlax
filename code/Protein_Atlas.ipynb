{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Protein_Atlas_(5) (1).ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "metadata": {
        "colab_type": "text",
        "id": "uqZTjPXN3WBX"
      },
      "cell_type": "markdown",
      "source": [
        "## Imports and constants"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "f0bjPvkOssWq",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import zipfile\n",
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "GxvWbivvJ4MI"
      },
      "cell_type": "markdown",
      "source": [
        "## Preprocessing stuff"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "1gzqr71StMcO",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def extract_datazip(zipfile_path=None, validation_start_index=None, train_percent=0.95, extract_path='data', save_filelist=True):\n",
        "    \"\"\"\n",
        "    Extract dataset train and test zip file present in 'zipfile_path' path to train, valid and test directory in 'extract_path' path.\n",
        "    Creating directory structure:\n",
        "        # TODO: find directory structure\n",
        "        \n",
        "    return:\n",
        "        list, list, list : list of files in train, valid and test directories\n",
        "        dict : dict of paths \n",
        "    \n",
        "    \"\"\"\n",
        "    \n",
        "    assert os.path.isfile(os.path.join(zipfile_path, 'train.zip')), \"{} file not found\".format(os.path.join(zipfile_path,'train.zip'))\n",
        "    assert os.path.isfile(os.path.join(zipfile_path,'test.zip')), \"{} file not found\".format(os.path.join(zipfile_path,'test.zip'))\n",
        "    # create directories\n",
        "    if not os.path.isdir(extract_path): \n",
        "        os.makedirs(extract_path)\n",
        "        os.makedirs(os.path.join(extract_path,'train'))\n",
        "        os.makedirs(os.path.join(extract_path,'valid'))\n",
        "        os.makedirs(os.path.join(extract_path,'test'))\n",
        "    \n",
        "    train_zfile = zipfile.ZipFile(os.path.join(zipfile_path,'train.zip'))\n",
        "    test_zfile = zipfile.ZipFile(os.path.join(zipfile_path,'test.zip'))\n",
        "    train_filelist = train_zfile.namelist()\n",
        "    train_filelist = pd.DataFrame(train_filelist, columns=['fileid'])\n",
        "    train_filelist = train_filelist.sort_values(by='fileid')\n",
        "    train_filelist = train_filelist.reset_index(drop=True)\n",
        "    \n",
        "    filelist = pd.DataFrame(train_filelist.fileid.str.split(\"_\",1).tolist(),columns=[\"id\",\"color\"] )\n",
        "    \n",
        "    \n",
        "    if validation_start_index ==None:\n",
        "        validation_start_index = int(train_filelist.shape[0]//4*train_percent)*4\n",
        "    \n",
        "    if save_filelist:\n",
        "        save_list_validation_start_index = validation_start_index//4\n",
        "        if validation_start_index == None:\n",
        "            save_list_validation_start_index = int(save_list.shape[0]*train_percent)\n",
        "        \n",
        "        save_list = pd.DataFrame(train_filelist.fileid.str.split('_', 1).tolist(), columns=['fileid','color']).drop(columns='color').drop_duplicates()        \n",
        "        save_list[ : save_list_validation_start_index].to_csv(os.path.join(extract_path,'train/train_filelist.csv'), sep=',', index=False)\n",
        "        save_list[save_list_validation_start_index : ].to_csv(os.path.join(extract_path,'valid/valid_filelist.csv'), sep=',', index=False)\n",
        "        test_savelist = pd.DataFrame(test_zfile.namelist(), columns=[\"fileid\"]).sort_values(by=\"fileid\").reset_index(drop=True)\n",
        "        test_savelist = pd.DataFrame(test_savelist.fileid.str.split(\"_\",1).tolist(),columns=[\"id\",\"color\"] ).drop(columns='color').drop_duplicates()\n",
        "        test_savelist.to_csv(os.path.join(extract_path,'test/test_filelist.csv'), sep=',', index=False)\n",
        "    \n",
        "    valid_filelist = np.squeeze(train_filelist[validation_start_index : ].values)\n",
        "    train_filelist = np.squeeze(train_filelist[ : validation_start_index].values)\n",
        "\n",
        "\n",
        "    print('Extracting train images at {}'.format(extract_path), end='\\t')\n",
        "    train_zfile.extractall(os.path.join(extract_path, 'train'), train_filelist)\n",
        "    print('done')\n",
        "    print('Extracting validation images at {}'.format(extract_path), end='\\t')\n",
        "    train_zfile.extractall(os.path.join(extract_path, 'valid'), valid_filelist)\n",
        "    print('done')\n",
        "    print('Extracting test images at {}'.format(extract_path), end='\\t')\n",
        "    test_zfile.extractall(os.path.join(extract_path, 'test'))\n",
        "    print('done')\n",
        "    extract_path = os.path.dirname(os.path.abspath(extract_path))\n",
        "    return {'train': os.path.join(extract_path,'train'), 'valid': os.path.join(extract_path,'valid'), 'test': os.path.join(extract_path,'test')}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "cellView": "both",
        "colab_type": "code",
        "id": "pHzLhNuvoeGQ",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def preprocess_label(label_file=None, save_file=None):\n",
        "    \"\"\"\n",
        "    Converts label csv file into onehot format, requires pandas dataframe\n",
        "    \"\"\"\n",
        "    \n",
        "    new_lbl = pd.DataFrame(data=None, columns=['Id']+ [i for i in range(28)])\n",
        "    tr_list = pd.read_csv(label_file, sep=',')\n",
        "    for index, row in tr_list.iterrows():\n",
        "        n_hot = np.array([0]*28)\n",
        "        n_hot[list(map(int,row['Target'].split()))] = 1\n",
        "        new_lbl.loc[index]= [row[\"Id\"]]+ n_hot.tolist()\n",
        "    if save_file:\n",
        "        new_lbl.to_csv(save_file, index=False)\n",
        "    return new_lbl\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "aa8epWXwCgxI",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def split_label(label, validation_start_index=None, train_percent=0.95, save_listfile=True, save_path='data'):\n",
        "    \"\"\"\n",
        "    Split label file into train and validation \n",
        "    \n",
        "    \"\"\"\n",
        "    lbl = pd.read_csv(label)\n",
        "    if validation_start_index == None:\n",
        "        validation_start_index = int(lbl.shape[0] * train_percent)\n",
        "    if save_listfile:\n",
        "        lbl[ : validation_start_index].to_csv(os.path.join(save_path, 'train_label.csv'), index=False)\n",
        "        lbl[validation_start_index : ].to_csv(os.path.join(save_path, 'valid_label.csv'), index=False)\n",
        "    return np.squeeze(lbl[ : validation_start_index].values), np.squeeze(lbl[validation_start_index : ].values)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "Ww5vAu_DQYun"
      },
      "cell_type": "markdown",
      "source": [
        "## Make Iterators"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "Wbdj-amsxBQV",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_batch(img_file_path='data', batch_type=None, lbl_dir_path ='./', image_dim=[512, 512, 1], batch_size=32):\n",
        "    '''\n",
        "    Get iterator of dataset\n",
        "    \n",
        "    Parameters:\n",
        "    img_file_path - directory path containing train, valid and test image directories\n",
        "    batch_type - either 'train', 'valid' or 'test'\n",
        "    lbl_dir_path - directory path containing train, valid and test label csv file\n",
        "    image_dim - Three dimensions of image [width, height, channel]\n",
        "    batch_size - batch size of dataset\n",
        "    \n",
        "    Returns:\n",
        "    train and validation iterator if batch type is train\n",
        "    else test iterator\n",
        "    '''\n",
        "    \n",
        "    def decode_img(file_path, filename, image_dim):\n",
        "        r = tf.reshape(tf.image.decode_png(tf.read_file(file_path+'/'+filename + '_red.png'), channels=1), image_dim)\n",
        "        g = tf.reshape(tf.image.decode_png(tf.read_file(file_path+'/'+filename + '_green.png'), channels=1), image_dim)\n",
        "        b = tf.reshape(tf.image.decode_png(tf.read_file(file_path+'/'+filename + '_blue.png'), channels=1), image_dim)\n",
        "        y = tf.reshape(tf.image.decode_png(tf.read_file(file_path+'/'+filename + '_yellow.png'), channels=1), image_dim)\n",
        "        img = tf.concat([r,g,b,y], axis=2)\n",
        "        return img/ 255\n",
        "    \n",
        "    assert batch_type.lower() in ['train', 'valid','test'], 'type should be train or test'\n",
        "    if batch_type.lower() in ['train','valid']:\n",
        "        train_img_path=os.path.join(img_file_path, batch_type)\n",
        "        train_lbl_path=os.path.join(lbl_dir_path, '')\n",
        "        tr_lbl_mat = pd.read_csv(train_lbl_path+'/{}_label.csv'.format(batch_type), sep=',').drop('Id', axis=1).values        \n",
        "        tr_img = tf.data.TextLineDataset(filenames=[train_img_path+'/{}_filelist.csv'.format(batch_type)]).skip(1)\n",
        "        tr_lbl = tf.data.Dataset.from_tensor_slices(tr_lbl_mat)\n",
        "        tr_dataset = tf.data.Dataset.zip((tr_img,tr_lbl))\n",
        "        tr_dataset = tr_dataset.shuffle(buffer_size=50000).map(lambda x,y: (x,decode_img(train_img_path, x, image_dim), y))\n",
        "        tr_dataset = tr_dataset.batch(batch_size).prefetch(batch_size)\n",
        "        return tr_dataset.make_initializable_iterator()\n",
        "    else:\n",
        "        test_img_path=os.path.join(img_file_path, 'test')\n",
        "        te_dataset = tf.data.TextLineDataset(filenames=[test_img_path+'/test_filelist.csv']).skip(1)\n",
        "        te_dataset = te_dataset.map(lambda x: [x,decode_img(test_img_path, x, image_dim)])\n",
        "        te_dataset = te_dataset.batch(batch_size).prefetch(batch_size)\n",
        "        return te_dataset.make_initializable_iterator()\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "kyIxxqEcxLht"
      },
      "cell_type": "markdown",
      "source": [
        "## Model Definition"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "Bbf7N0tjnWY2",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class SimpleCNN:\n",
        "    def __init__(self,X, output_units):\n",
        "        self.layer = tf.layers.conv2d(inputs=X, filters=16, kernel_size=[5,5], kernel_initializer= tf.initializers.random_uniform(),\t)\n",
        "        self.layer = tf.layers.max_pooling2d(inputs = self.layer, pool_size=[2,2], strides=2)\n",
        "        self.layer = tf.nn.relu(self.layer)\n",
        "        \n",
        "        self.layer = tf.layers.conv2d(inputs=self.layer, filters=32, kernel_size=[3,3], kernel_initializer= tf.initializers.random_uniform(), )\n",
        "        self.layer = tf.layers.max_pooling2d(inputs = self.layer, pool_size=[2,2], strides=2)\n",
        "        self.layer = tf.nn.relu(self.layer)\n",
        "        \n",
        "        self.layer = tf.layers.conv2d(inputs=self.layer, filters=64, kernel_size=[3,3], kernel_initializer= tf.initializers.random_uniform(), )\n",
        "        self.layer = tf.layers.max_pooling2d(inputs = self.layer, pool_size=[2,2], strides=2)\n",
        "        self.layer = tf.nn.relu(self.layer)\n",
        "        \n",
        "        self.layer = tf.layers.flatten(inputs = self.layer,)\n",
        "        print(self.layer)\n",
        "        self.layer = tf.layers.dense(inputs= self.layer, units = output_units, kernel_initializer=tf.initializers.random_uniform(), )\n",
        "    \n",
        "    def get_NN(self):\n",
        "        return self.layer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "EScYnXSanWY4",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# https://arxiv.org/pdf/1608.06993.pdf\n",
        "class DenseNet:\n",
        "    \n",
        "    #DenseNet-121 DenseNet-169 DenseNet-201 DenseNet-264\n",
        "    nw_types = {121:[6, 12, 24, 16],\n",
        "                169:[6, 12, 32, 32],\n",
        "                201:[6, 12, 48, 32],\n",
        "                264:[6, 12, 64, 48],}\n",
        "        \n",
        "    def __init__(self, x, nb_blocks, nw_type, n_class, filters, training):\n",
        "        self.dropout_rate = 0.2\n",
        "        self.nb_blocks = nb_blocks\n",
        "        self.nw_type = nw_type\n",
        "        self.filters = filters\n",
        "        self.training = training\n",
        "        self.n_class = n_class\n",
        "        self.model = self._dense_net(x)\n",
        "        \n",
        "        \n",
        "    def get_model(self):\n",
        "        return self.model\n",
        "\n",
        "    def _bottleneck_layer(self, x, scope):\n",
        "        with tf.name_scope(scope):\n",
        "            x = tf.layers.batch_normalization(x,  training=self.training, name=scope+'_0_bn')\n",
        "            x = tf.nn.relu(x, name=scope+'_0_relu')\n",
        "            x = tf.layers.conv2d(x, filters=4 * self.filters, kernel_size=[1,1], padding='same', name=scope+'_0_conv')\n",
        "            x = tf.layers.dropout(x, rate=self.dropout_rate, training=self.training, name=scope+'_0_drop')\n",
        "\n",
        "            x = tf.layers.batch_normalization(x, training=self.training, name=scope+'_1_bn')\n",
        "            x = tf.nn.relu(x, name=scope+'_1_relu')\n",
        "            x = tf.layers.conv2d(x, filters=self.filters, kernel_size=[3,3], padding='same', name=scope+'_1_conv')\n",
        "            x = tf.layers.dropout(x, rate=self.dropout_rate, training=self.training, name=scope+'_1_drop')\n",
        "            return x\n",
        "\n",
        "    def _transition_layer(self, x, scope):\n",
        "        with tf.name_scope(scope):\n",
        "            x = tf.layers.batch_normalization(x,  training=self.training, name=scope+'_bn')\n",
        "            x = tf.nn.relu(x, name=scope+'_relu')\n",
        "            x = tf.layers.conv2d(x, filters=self.filters, kernel_size=[1,1], padding='same', name=scope+'_conv')\n",
        "            x = tf.layers.dropout(x, rate=self.dropout_rate, training=self.training, name=scope+'_drop')\n",
        "            x = tf.layers.average_pooling2d(x, pool_size=[2,2], strides=2, name=scope+'_pool')\n",
        "            return x\n",
        "    \n",
        "    def _global_average_pooling2d(self, inputs, data_format='channels_last', keepdims=False, name='avg_pool'):\n",
        "        \n",
        "        assert data_format.lower() in ['channels_last', 'channels_first'], \"incorrect dataformat: should be either of ['channels_last', 'channels_first']\"\n",
        "        \n",
        "        if data_format=='channels_last':\n",
        "            return tf.reduce_mean(inputs, axis=[1,2], keepdims=keepdims, name=name )\n",
        "        else:\n",
        "            return tf.reduce_mean(inputs, axis=[2,3], keepdims=keepdims, name=name )\n",
        "\n",
        "    def _dense_block(self, input_x, nb_layers, layer_name):\n",
        "        with tf.name_scope(layer_name):\n",
        "            layers_concat = list()\n",
        "            layers_concat.append(input_x)\n",
        "\n",
        "            x = self._bottleneck_layer(input_x, scope=layer_name + '_block' + str(0))\n",
        "\n",
        "            layers_concat.append(x)\n",
        "\n",
        "            for i in range(nb_layers - 1):\n",
        "                x = tf.concat(layers_concat, axis=3)\n",
        "                x = self._bottleneck_layer(x, scope=layer_name + '_block' + str(i + 1))\n",
        "                layers_concat.append(x)\n",
        "\n",
        "            x = tf.concat(layers_concat, axis=3)\n",
        "\n",
        "            return x\n",
        "\n",
        "    def _dense_net(self, input_x):\n",
        "        x = tf.layers.conv2d(input_x, filters=2 * self.filters, kernel_size=[7,7], strides=2, name='conv1/conv')\n",
        "        x = tf.layers.max_pooling2d(x, pool_size=[3,3], strides=2, name='max_pool0')\n",
        "        \n",
        "        if self.nw_type in self.nw_types.keys():\n",
        "            \n",
        "            for i, layers in enumerate(self.nw_types[self.nw_type][:-1]) :\n",
        "                # 6 -> 12 -> 48\n",
        "                x = self._dense_block(input_x=x, nb_layers=layers, layer_name='dense_'+str(i))\n",
        "                x = self._transition_layer(x, scope='trans_'+str(i))\n",
        "                \n",
        "            x = self._dense_block(input_x=x, nb_layers=self.nw_types[self.nw_type][-1], layer_name='dense_final')\n",
        "            \n",
        "        else:\n",
        "            # default to \n",
        "            x = self._dense_block(input_x=x, nb_layers=6, layer_name='dense_1')\n",
        "            x = self._transition_layer(x, scope='trans_1')\n",
        "\n",
        "            x = self._dense_block(input_x=x, nb_layers=12, layer_name='dense_2')\n",
        "            x = self._transition_layer(x, scope='trans_2')\n",
        "\n",
        "            x = self._dense_block(input_x=x, nb_layers=24, layer_name='dense_3')\n",
        "            x = self._transition_layer(x, scope='trans_3')\n",
        "            x = self._dense_block(input_x=x, nb_layers=16, layer_name='dense_final')\n",
        "            \n",
        "        x = self._global_average_pooling2d(x, name='global_avg_pool')\n",
        "        x = tf.layers.flatten(x, name='flat')\n",
        "        x = tf.layers.dense(inputs=x, units=self.n_class, name='fully_connected')\n",
        "\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "Qa2mWJe_NwXR"
      },
      "cell_type": "markdown",
      "source": [
        "## Run"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "Pl-GBCNAtwKo",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#constants\n",
        "\n",
        "kwargs=dict(\n",
        "    validation_start_partition = 0.9, \n",
        "    image_dir ='/mnt/disk2/proteinatlax/data',\n",
        "    label_dir ='/mnt/disk2/proteinatlax/data/labels',\n",
        "    batch_size=8,\n",
        "    lr = 0.01,\n",
        "    #lr = 0.1,\n",
        "    #decay_lr = True,\n",
        "    #decay_rate = 0.96,\n",
        "    #decay_step = 1000,\n",
        "    label_size = 28,\n",
        "    checkpoint_dir = '/mnt/disk2/proteinatlax/model',\n",
        "    break_patience = 10,\n",
        "    nw_type=121, \n",
        "    filters=12,\n",
        "    sigmoid_threshold=0.5,\n",
        "    epochs=1000\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "TsG_oBhJsE1H",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#path_dict = extract_datazip(zipfile_path='../',extract_path=kwargs['image_dir'])\n",
        "#processed_label = preprocess_label(label_file='../train.csv', save_file=os.path.join(kwargs['label_dir'],'processed_train.csv'))\n",
        "#tr_label, va_label = split_label(label=os.path.join(kwargs['label_dir'],'processed_train.csv'), save_path=kwargs['label_dir'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "L6tQ1M4onj9H"
      },
      "cell_type": "markdown",
      "source": [
        "### Setup graph"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "z3cNaBuDnjhg",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#tf graph\n",
        "graph = tf.Graph()\n",
        "with graph.as_default():\n",
        "\n",
        "    tr_summaries_dir       = os.path.join(kwargs['checkpoint_dir'], 'train')\n",
        "    va_summaries_dir       = os.path.join(kwargs['checkpoint_dir'], 'validation')\n",
        "    tr_checkpoint_prefix   = os.path.join( kwargs['checkpoint_dir'], 'model.ckpt')\n",
        "    best_checkpoint_prefix = os.path.join( kwargs['checkpoint_dir'], 'best_model/model.ckpt')\n",
        "    \n",
        "    #Dataset\n",
        "    tr_iter = get_batch( img_file_path=kwargs['image_dir'], batch_type='train', \n",
        "                        lbl_dir_path=kwargs['label_dir'], batch_size=kwargs['batch_size'] )\n",
        "    tr_id, tr_x, tr_y = tr_iter.get_next()\n",
        "    va_iter = get_batch( img_file_path=kwargs['image_dir'], batch_type='valid', \n",
        "                        lbl_dir_path=kwargs['label_dir'], batch_size=kwargs['batch_size'] )\n",
        "    va_id, va_x, va_y = va_iter.get_next()\n",
        "    \n",
        "    #placeholders\n",
        "    X = tf.placeholder(name='X', shape=[None,tr_x.shape[1],tr_x.shape[2],tr_x.shape[3]], dtype=tf.float32)\n",
        "    Y = tf.placeholder(name='Y', shape=[None, tr_y.shape[1]], dtype=tf.float32)\n",
        "    Y = tf.stop_gradient(Y)\n",
        "    #variables\n",
        "    \n",
        "    global_step = tf.Variable(0, name='global_step')\n",
        "    sig_cond = tf.Variable([kwargs['sigmoid_threshold']]*kwargs['label_size'])\n",
        "    \n",
        "    #model\n",
        "    logit = DenseNet(x=X, nb_blocks=None, nw_type=kwargs['nw_type'], \n",
        "                     n_class=kwargs['label_size'], filters=kwargs['filters'], training=True).get_model()\n",
        "    \n",
        "    Y_ = tf.nn.sigmoid(logit,)\n",
        "    Y_ = tf.cast(x = tf.greater(tf.cast(Y_, tf.float32), sig_cond), dtype=tf.int32, name = 'predictions')\n",
        "    \n",
        "    loss = tf.reduce_mean( tf.reduce_sum(tf.nn.sigmoid_cross_entropy_with_logits(labels=Y, logits=logit), axis=1))\n",
        "    f1_score, score_update = tf.contrib.metrics.f1_score( labels=Y, predictions=Y_, name='f1_score')\n",
        "    \n",
        "    score_op_init = tf.variables_initializer(tf.get_default_graph().get_collection('local_variables',), \n",
        "                                                name='metrics_initializer')\n",
        "    \n",
        "    if 'decay_lr' in kwargs.keys() and kwargs['decay_lr']:\n",
        "        lr = tf.train.exponential_decay(kwargs['lr'], global_step=global_step, decay_rate=kwargs['decay_rate'], decay_steps=kwargs['decay_steps'], name='lr_decay' )\n",
        "        tf.summary.scalar('Learning Rate', lr)\n",
        "    else: \n",
        "        lr = kwargs['lr']\n",
        "                \n",
        "    optimizer = tf.train.AdamOptimizer( learning_rate=lr, )\n",
        "    \n",
        "    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
        "    with tf.control_dependencies(update_ops):\n",
        "        train_op = optimizer.minimize(loss, global_step=global_step)\n",
        "    \n",
        "    # Model Summaries\n",
        "    tf.summary.scalar('Loss', loss)\n",
        "    tf.summary.scalar('F1_Score', f1_score)  \n",
        "    all_summaries      = tf.summary.merge_all()\n",
        "    tr_summary_writer  = tf.summary.FileWriter( tr_summaries_dir, graph )\n",
        "    val_summary_writer = tf.summary.FileWriter( va_summaries_dir, graph )\n",
        "    tr_saver           = tf.train.Saver(max_to_keep=10)\n",
        "    best_saver         = tf.train.Saver(max_to_keep=1)\n",
        "    config_proto       = tf.ConfigProto(allow_soft_placement=True)\n",
        "    tr_session         = tf.Session(graph=graph, config= config_proto )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kIuUI0yh0kgy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Training loop"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "bm9ydsr9vuxG",
        "outputId": "4f0d57b5-6dde-4308-a5bf-e2d1bc298d25",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# training step\n",
        "with graph.as_default():\n",
        "    with tf.name_scope('training_loop'):\n",
        "        \n",
        "        tr_session.run(tf.global_variables_initializer())\n",
        "        tr_saver.save(save_path=tr_checkpoint_prefix, sess=tr_session, global_step=global_step.eval(tr_session))\n",
        "        \n",
        "        # naive early stopping :P params\n",
        "        prev_loss = best_loss = 100000000000.0\n",
        "        cur_patience = 0 \n",
        "        \n",
        "        for epoc in range( kwargs['epochs'] ):\n",
        "            # one complete pass of training data\n",
        "            print(\"epoc: {}\".format(epoc) )\n",
        "            tr_session.run(tr_iter.initializer)\n",
        "            tr_session.run(score_op_init)\n",
        "            while True:\n",
        "                try:\n",
        "                    #train model\n",
        "                    tr_Id, tr_X, tr_Y = tr_session.run([tr_id, tr_x, tr_y])\n",
        "                    act,pred, tr_loss, _, tr_accuracy, _ , tr_summaries = tr_session.run([Y,Y_, loss, score_update, f1_score, \n",
        "                                                                                train_op, all_summaries],\n",
        "                                                                        feed_dict={X:tr_X, Y:tr_Y})\n",
        "                    #print(act, pred)\n",
        "                    print(\"global_step: {:6}, loss: {:13.6f}, accuracy ={:.6f}\".format(global_step.eval(tr_session), tr_loss, tr_accuracy), end='\\r')\n",
        "                    \n",
        "                    tr_summary_writer.add_summary(tr_summaries, global_step.eval(tr_session))\n",
        "\n",
        "                except tf.errors.OutOfRangeError:\n",
        "                    tr_saver.save(save_path=tr_checkpoint_prefix, sess=tr_session, \n",
        "                                  global_step=global_step.eval(tr_session))\n",
        "                    break\n",
        "                    \n",
        "            # one complete pass of validation data\n",
        "            tr_session.run(va_iter.initializer)\n",
        "            tr_session.run(score_op_init)\n",
        "            while True:\n",
        "                try:\n",
        "                    #validate model\n",
        "                    va_Id, va_X, va_Y = tr_session.run([va_id, va_x, va_y])\n",
        "                    val_loss, _, val_accuracy, val_summaries = tr_session.run([loss, score_update, \n",
        "                                                                               f1_score, all_summaries],\n",
        "                                                                             feed_dict={X:va_X, Y:va_Y})\n",
        "                except tf.errors.OutOfRangeError:\n",
        "                    val_summary_writer.add_summary(val_summaries, global_step.eval(tr_session))\n",
        "                    print(\"global_step: {:6}, loss: {:13.6f}, accuracy ={:.6f}, val_loss: {:13.6f}, val_accuracy: {:.6f}\".format(global_step.eval(tr_session), \n",
        "                                                                   tr_loss, tr_accuracy, val_loss, val_accuracy))\n",
        "                    break\n",
        "            # save checkpoint if loss is better than previous best loss\n",
        "            if float(\"{:.3f}\".format(val_loss)) < float(\"{:.3f}\".format(best_loss)):\n",
        "                best_loss = val_loss\n",
        "                cur_patience = 0\n",
        "                best_saver.save(save_path=best_checkpoint_prefix, sess=tr_session, \n",
        "                                global_step=global_step.eval(tr_session))\n",
        "            else:\n",
        "                cur_patience += 1\n",
        "\n",
        "            if cur_patience == kwargs['break_patience']:\n",
        "                print('\\n############ Early stopping ############')\n",
        "                tr_session.close()\n",
        "                break"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoc: 0\n",
            "global_step:   3690, loss:      4.667702, accuracy =0.271346, val_loss:                                  3.603684, val_accuracy: 0.279509\n",
            "epoc: 1\n",
            "global_step:   7380, loss:      4.524732, accuracy =0.323440, val_loss:                                  8.385436, val_accuracy: 0.335511\n",
            "epoc: 2\n",
            "global_step:  11070, loss:      5.644957, accuracy =0.362442, val_loss:                                  3.277678, val_accuracy: 0.457942\n",
            "epoc: 3\n",
            "global_step:  14760, loss:      4.071045, accuracy =0.403178, val_loss:                                  2.428470, val_accuracy: 0.454779\n",
            "epoc: 4\n",
            "global_step:  18450, loss:      4.816981, accuracy =0.442238, val_loss:                                  5.741456, val_accuracy: 0.483895\n",
            "epoc: 5\n",
            "global_step:  22140, loss:      3.601640, accuracy =0.469573, val_loss:                                  2.313452, val_accuracy: 0.426277\n",
            "epoc: 6\n",
            "global_step:  25830, loss:      4.246340, accuracy =0.493176, val_loss:                                  4.693980, val_accuracy: 0.495074\n",
            "epoc: 7\n",
            "global_step:  29520, loss:      2.956824, accuracy =0.514406, val_loss:                                  4.456381, val_accuracy: 0.520371\n",
            "epoc: 8\n",
            "global_step:  33210, loss:      3.330956, accuracy =0.532821, val_loss:                                  2.076056, val_accuracy: 0.532745\n",
            "epoc: 9\n",
            "global_step:  36900, loss:      4.154942, accuracy =0.536442, val_loss:                                  2.397887, val_accuracy: 0.498170\n",
            "epoc: 10\n",
            "global_step:  39186, loss:      3.015165, accuracy =0.546190\r"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "tW7rntL10kg4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Generate prediction"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "LGhixXP3445i",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "with tf.device(\"/device:GPU:0\"):\n",
        "        with tf.name_scope('test_loop'):\n",
        "            with graph.as_default():\n",
        "                with tf.variable_scope('test_graph'):    \n",
        "                    te_iter = get_batch( img_file_path=kwargs['image_dir'], batch_type='test', lbl_dir_path=kwargs['label_dir'], batch_size=kwargs['test_batch_size'] )\n",
        "                    te_img_ids, te_x, te_y = te_iter.get_next()\n",
        "                    \n",
        "                    graph.run(te_iter.initializer)\n",
        "                    while True:\n",
        "                        try:\n",
        "                            #validate model\n",
        "                            predictions = graph.run(Y_, {X:tr_x})\n",
        "                            all_predictions.append( zip(te_img_ids,predictions) )\n",
        "                        except tf.errors.OutOfRangeError:\n",
        "                            with open(os.path.join(checkpoint_dir, 'predictions.csv')) as fp:\n",
        "                                fp.write(all_predictions)\n",
        "                            break\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pInIendk0kg7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}